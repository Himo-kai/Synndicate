#!/usr/bin/env python3
"""
Comprehensive integration test for Synndicate AI system.
Tests the full pipeline: Models + RAG + Agents + Orchestrator
"""

import asyncio
import os
import sys

# Add src to path
# Use editable install: pip install -e .
from synndicate.agents.coder import CoderAgent
from synndicate.agents.critic import CriticAgent
from synndicate.agents.planner import PlannerAgent
from synndicate.models.interfaces import ModelConfig, ModelFormat, ModelType
from synndicate.models.manager import ModelManager
from synndicate.rag.chunking import SemanticChunker
from synndicate.rag.context import ContextBuilder, ContextIntegrator, ContextStrategy
from synndicate.rag.indexer import DocumentIndexer
from synndicate.rag.retriever import QueryContext, RAGRetriever, SearchMode


async def test_openai_fallback():
    """Test OpenAI API as fallback language model."""
    print("ğŸ”§ Testing OpenAI API Fallback...")

    # Check if OpenAI API key is available
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("  âš ï¸  No OpenAI API key found (set OPENAI_API_KEY environment variable)")
        return None

    try:
        # Create OpenAI model config
        config = ModelConfig(
            name="gpt-3.5-turbo",
            model_type=ModelType.LANGUAGE_MODEL,
            format=ModelFormat.OPENAI_API,
            path="gpt-3.5-turbo",
            parameters={
                "api_key": api_key,
                "max_tokens": 150,
                "temperature": 0.7,
            },
        )

        # Test with model manager
        manager = ModelManager()
        manager._model_configs["gpt-3.5-turbo"] = config

        await manager.load_language_model("gpt-3.5-turbo")

        # Test generation
        response = await manager.generate_text(
            "What is artificial intelligence? Answer in one sentence.", model_name="gpt-3.5-turbo"
        )

        print(f"  âœ… OpenAI API working: {response.content[:100]}...")
        await manager.shutdown()
        return True

    except Exception as e:
        print(f"  âŒ OpenAI API test failed: {e}")
        return False


async def test_rag_pipeline():
    """Test complete RAG pipeline."""
    print("\nğŸ“š Testing RAG Pipeline...")

    try:
        # Initialize model manager with embedding model
        manager = ModelManager()
        await manager.initialize()

        # Create test documents
        documents = [
            "Python is a high-level programming language known for its simplicity and readability. It's widely used in web development, data science, and artificial intelligence.",
            "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.",
            "The Synndicate AI system is designed to orchestrate multiple AI agents working together to solve complex coding tasks through planning, implementation, and review.",
            "Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language, enabling machines to understand and generate text.",
        ]

        # Step 1: Chunking
        chunker = SemanticChunker(max_chunk_size=200, overlap=50)
        all_chunks = []

        for i, doc in enumerate(documents):
            chunks = chunker.chunk(doc, metadata={"doc_id": i, "source": f"test_doc_{i}"})
            all_chunks.extend(chunks)

        print(f"  âœ… Created {len(all_chunks)} chunks from {len(documents)} documents")

        # Step 2: Indexing
        indexer = DocumentIndexer(
            embedding_function=lambda texts: manager.generate_embeddings(texts), batch_size=10
        )

        await indexer.add_chunks(all_chunks)
        print(f"  âœ… Indexed {len(all_chunks)} chunks")

        # Step 3: Retrieval
        retriever = RAGRetriever(
            chunks=all_chunks, embedding_function=lambda texts: manager.generate_embeddings(texts)
        )

        # Test query
        query = "How does machine learning work in AI systems?"
        query_context = QueryContext(
            query=query,
            conversation_history=[],
            agent_context={"agent_type": "planner", "task": "research"},
        )

        results = await retriever.retrieve(query_context, SearchMode.HYBRID, max_results=3)
        print(f"  âœ… Retrieved {len(results)} relevant chunks")

        # Step 4: Context Integration
        context_builder = ContextBuilder()
        context_integrator = ContextIntegrator()

        context = context_builder.build_context(
            results, max_tokens=500, strategy=ContextStrategy.CONCATENATE
        )

        context_integrator.format_for_agent(
            context, agent_type="planner", query=query
        )

        print(f"  âœ… Built context ({len(context.content)} chars) for agent")
        print(f"  ğŸ“ Context preview: {context.content[:100]}...")

        await manager.shutdown()
        return True

    except Exception as e:
        print(f"  âŒ RAG pipeline test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_agent_integration():
    """Test agent integration with models and RAG."""
    print("\nğŸ¤– Testing Agent Integration...")

    try:
        # Initialize components
        manager = ModelManager()
        await manager.initialize()

        # Create a simple task
        task = "Create a Python function that calculates the factorial of a number"

        # Test with embedding model (agents can use embeddings for context)
        embeddings = await manager.generate_embeddings([task])
        print(f"  âœ… Generated task embedding (dimension: {len(embeddings[0])})")

        # Test agent creation (without full language model for now)
        try:
            PlannerAgent(model_manager=manager, embedding_model_name="bge-small-en-v1.5")
            print("  âœ… Created planner agent")

            CoderAgent(model_manager=manager, embedding_model_name="bge-small-en-v1.5")
            print("  âœ… Created coder agent")

            CriticAgent(model_manager=manager, embedding_model_name="bge-small-en-v1.5")
            print("  âœ… Created critic agent")

        except Exception as e:
            print(f"  âš ï¸  Agent creation test skipped (expected without language model): {e}")

        await manager.shutdown()
        return True

    except Exception as e:
        print(f"  âŒ Agent integration test failed: {e}")
        return False


async def test_end_to_end_workflow():
    """Test a simplified end-to-end workflow."""
    print("\nğŸ”„ Testing End-to-End Workflow...")

    try:
        # This would be a full workflow test if we had language models
        # For now, we'll test the components that are working

        manager = ModelManager()
        await manager.initialize()

        # Simulate a coding task workflow
        task = "Write a function to reverse a string"

        # 1. Task analysis (using embeddings)
        task_embedding = await manager.generate_embeddings([task])
        print("  âœ… Task analyzed (embedding generated)")

        # 2. Knowledge retrieval (RAG)
        knowledge_base = [
            "String reversal can be done using slicing: s[::-1]",
            "Python functions are defined with the def keyword",
            "Good functions should have docstrings and type hints",
        ]

        kb_embeddings = await manager.generate_embeddings(knowledge_base)
        print(f"  âœ… Knowledge base processed ({len(kb_embeddings)} entries)")

        # 3. Context preparation
        # This would normally feed into language model generation
        context = {
            "task": task,
            "knowledge": knowledge_base,
            "task_embedding": task_embedding[0][:5],  # Preview
        }

        print("  âœ… Context prepared for generation")
        print(f"  ğŸ“ Context: {context}")

        await manager.shutdown()
        return True

    except Exception as e:
        print(f"  âŒ End-to-end workflow test failed: {e}")
        return False


async def main():
    """Run all integration tests."""
    print("ğŸ§ª Starting Comprehensive Integration Tests\n")

    # Test results
    results = {}

    try:
        # Test OpenAI fallback
        results["openai"] = await test_openai_fallback()

        # Test RAG pipeline
        results["rag"] = await test_rag_pipeline()

        # Test agent integration
        results["agents"] = await test_agent_integration()

        # Test end-to-end workflow
        results["workflow"] = await test_end_to_end_workflow()

        # Summary
        print("\nğŸ“Š Integration Test Summary:")
        print(
            f"  ğŸ”§ OpenAI Fallback: {'âœ…' if results['openai'] else 'âŒ' if results['openai'] is False else 'âš ï¸ '}"
        )
        print(f"  ğŸ“š RAG Pipeline: {'âœ…' if results['rag'] else 'âŒ'}")
        print(f"  ğŸ¤– Agent Integration: {'âœ…' if results['agents'] else 'âŒ'}")
        print(f"  ğŸ”„ End-to-End Workflow: {'âœ…' if results['workflow'] else 'âŒ'}")

        # Overall assessment
        core_working = results["rag"] and results["agents"] and results["workflow"]

        if core_working:
            print("\nğŸ‰ Core integration is working successfully!")
            print("\nğŸ’¡ System Status:")
            print("  âœ… Embedding model (BGE) - Fully operational")
            print("  âœ… RAG subsystem - Fully operational")
            print("  âœ… Agent framework - Ready (needs language model)")
            print("  âœ… Model manager - Fully operational")

            if results["openai"]:
                print("  âœ… OpenAI API - Available as language model")
            else:
                print("  âš ï¸  Language model - Needs setup (see docs/MODEL_SETUP.md)")

            print("\nğŸš€ Ready for:")
            print("  - RAG-powered information retrieval")
            print("  - Agent-based task orchestration (with language model)")
            print("  - Full AI coding workflows (with language model)")

            print("\nğŸ“‹ Next Steps:")
            if not results["openai"]:
                print("  1. Set up language model (see docs/MODEL_SETUP.md)")
                print("  2. Download GGUF model or set OPENAI_API_KEY")
            print("  3. Test full agent workflows")
            print("  4. Build API and CLI interfaces")

            return 0
        else:
            print("\nâŒ Some core components failed")
            return 1

    except Exception as e:
        print(f"\nğŸ’¥ Integration test suite failed: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
